# TODO include MagNet once there is a robustml implementation for it
#-
  #name: "MagNet: a Two-Pronged Defense against Adversarial Examples"
  #url: https://arxiv.org/abs/1705.09064
  #authors: Meng & Chen
  #code: https://github.com/Trevillie/MagNet
  #venue: CCS 2017
  #venue_date: 2017-10-30
  #dataset: CIFAR-10
  #threat_model: $$\ell_2 (\epsilon = 0.5)$$
  #natural: 91% accuracy
  #claims: >
    #83% accuracy (on images originally classified correctly)
  #analyses:
    #- claims: 0% accuracy
      #citation: CW17
      #url: https://arxiv.org/abs/1711.08478
      #code: https://github.com/carlini/MagNet
# TODO include Efficient Defenses once there is a robustml implementation for it
#-
  #name: "Efficient Defenses Against Adversarial Attacks"
  #url: https://arxiv.org/abs/1707.06728
  #authors: Zantedeschi et al.
  #code: https://github.com/carlini/breaking_efficient_defenses
  #venue: AISec 2017
  #venue_date: 2017-11-03
  #dataset: CIFAR-10
  #threat_model: $$\ell_2 (\epsilon = \text{unspecified})$$
  #natural: 75% accuracy
  #claims: >
    #60% accuracy
  #analyses:
    #- claims: 0% accuracy
      #citation: CW17
      #url: https://arxiv.org/abs/1711.08478
      #code: https://github.com/carlini/breaking_efficient_defenses
-
  name: Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks
  url: https://arxiv.org/abs/1511.04508
  authors: Papernot et al.
  code: https://github.com/lengstrom/defensive-distillation
  venue: S&P 2016
  venue_date: 2016-05-23
  claims:
    - dataset: MNIST
      threat_model: $$\ell_0 (\epsilon = 112)$$
      natural: 99.51% accuracy
      claim: 0.45% adversary success rate in changing classifier's prediction
      analyses:
        - claim: 3.6% accuracy
          citation: CW16
          url: https://arxiv.org/abs/1608.04644
          code: https://github.com/lengstrom/defensive-distillation
-
  name: Deflecting Adversarial Attacks with Pixel Deflection
  url: https://arxiv.org/abs/1801.08926
  authors: Prakash et al.
  code: https://github.com/anishathalye/pixel-deflection
  venue: CVPR 2018
  venue_date: 2018-06-19
  claims:
    - dataset: ImageNet
      threat_model: $$\ell_2 (\epsilon = 0.05)$$
      natural: 98.9% accuracy (on images originally classified correctly by underlying model)
      claim: 81% accuracy (on images originally classified correctly)
      analyses:
        - claim: 0% accuracy
          citation: AC18
          url: https://arxiv.org/abs/1804.03286
          code: https://github.com/anishathalye/pixel-deflection
-
  name: Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser
  url: https://arxiv.org/abs/1712.02976
  authors: Liao et al.
  code: https://github.com/anishathalye/Guided-Denoise
  venue: CVPR 2018
  venue_date: 2018-06-19
  claims:
    - dataset: ImageNet
      threat_model: $$\ell_\infty (\epsilon = 4/255)$$
      natural: 75% accuracy
      claim: 75% accuracy
      analyses:
        - claim: 0% accuracy
          citation: AC18
          url: https://arxiv.org/abs/1804.03286
          code: https://github.com/anishathalye/guided-denoise
-
  name: Towards Deep Learning Models Resistant to Adversarial Attacks
  url: https://arxiv.org/abs/1706.06083
  authors: Madry et al.
  code: https://github.com/MadryLab/cifar10_challenge
  venue: ICLR 2018
  venue_date: 2018-04-30
  claims:
    - dataset: CIFAR&#8209;10
      threat_model: $$\ell_\infty (\epsilon = 8/255)$$
      natural: 87% accuracy
      claim: 46% accuracy
-
  name: Provable defenses against adversarial examples via the convex outer adversarial polytope
  url: https://arxiv.org/abs/1711.00851
  authors: Wong & Kolter
  code: https://github.com/dtsip/convex_adversarial
  venue: ICML 2018
  venue_date: 2018-07-11
  claims:
    - dataset: MNIST
      threat_model: $$\ell_\infty (\epsilon = 0.1)$$
      natural: 98.2% accuracy
      claim: 94.2% accuracy
-
  name: Mitigating Adversarial Effects Through Randomization
  url: https://arxiv.org/abs/1711.01991
  authors: Xie et al.
  code: https://github.com/anishathalye/obfuscated-gradients/tree/master/randomization
  venue: ICLR 2018
  venue_date: 2018-04-30
  claims:
    - dataset: ImageNet
      threat_model: $$\ell_\infty (\epsilon = 10/255)$$
      natural: 99.2% accuracy (on images originally classified correctly by underlying model)
      claim: 86% accuracy (on images originally classified correctly)
      analyses:
        - claim: 0% accuracy
          citation: ACW18
          url: https://arxiv.org/abs/1802.00420
          code: https://github.com/anishathalye/obfuscated-gradients/tree/master/randomization
-
  name: "Thermometer Encoding: One Hot Way To Resist Adversarial Examples"
  url: https://openreview.net/forum?id=S18Su--CW
  authors: Buckman et al.
  code: https://github.com/anishathalye/obfuscated-gradients/tree/master/thermometer
  venue: ICLR 2018
  venue_date: 2018-04-30
  claims:
    - dataset: CIFAR&#8209;10
      threat_model: $$\ell_\infty (\epsilon = 8/255)$$
      natural: 90% accuracy
      claim: 79% accuracy
      # adversarially trained variant
      analyses:
        - claim: 30% accuracy
          citation: ACW18
          url: https://arxiv.org/abs/1802.00420
          code: https://github.com/anishathalye/obfuscated-gradients/tree/master/thermometer
-
  name: Countering Adversarial Images using Input Transformations
  url: https://arxiv.org/abs/1711.00117
  authors: Guo et al.
  code: https://github.com/anishathalye/obfuscated-gradients/tree/master/inputtransformations
  venue: ICLR 2018
  venue_date: 2018-04-30
  claims:
    - dataset: ImageNet
      threat_model: $$\ell_2 (\epsilon = 0.06)$$
      natural: 75% accuracy
      claim: 70% accuracy on ImageNet with average normalized $$\ell_2$$ perturbation of 0.06
      analyses:
        - claim: 0% accuracy
          citation: ACW18
          url: https://arxiv.org/abs/1802.00420
          code: https://github.com/anishathalye/obfuscated-gradients/tree/master/inputtransformations
-
  name: Stochastic Activation Pruning for Robust Adversarial Defense
  url: https://arxiv.org/abs/1803.01442
  authors: Dhillon et al.
  code: https://github.com/anishathalye/obfuscated-gradients/tree/master/sap
  venue: ICLR 2018
  venue_date: 2018-04-30
  claims:
    - dataset: CIFAR&#8209;10
      threat_model: $$\ell_\infty (\epsilon = 4/255)$$
      natural: 83% accuracy
      claim: 51% accuracy
      analyses:
        - claim: 0% accuracy
          citation: ACW18
          url: https://arxiv.org/abs/1802.00420
          code: https://github.com/anishathalye/obfuscated-gradients/tree/master/sap
-
  name: "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples"
  url: https://arxiv.org/abs/1710.10766
  authors: Song et al.
  code: https://github.com/anishathalye/obfuscated-gradients/tree/master/pixeldefend
  venue: ICLR 2018
  venue_date: 2018-04-30
  claims:
    - dataset: CIFAR&#8209;10
      threat_model: $$\ell_\infty (\epsilon = 8/255)$$
      natural: 90% accuracy
      claim: 70% accuracy
      analyses:
        - claim: 9% accuracy
          citation: ACW18
          url: https://arxiv.org/abs/1802.00420
          code: https://github.com/anishathalye/obfuscated-gradients/tree/master/pixeldefend
# TODO include Defense-GAN once there is a robustml implementation for it
#-
  #name: "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models"
  #url: https://arxiv.org/abs/1805.06605
  #authors: Samangouei et al.
  #code: https://github.com/anishathalye/obfuscated-gradients/tree/master/defensegan
  #venue: ICLR 2018
  #venue_date: 2018-04-30
  #dataset: MNIST
  #threat_model: $$\ell_2 (\epsilon = \text{unspecified})$$
  #natural: 99.7% accuracy
  #claims: >
    #92% accuracy
  #analyses:
    #- claims: 55% accuracy
      #citation: ACW18
      #url: https://arxiv.org/abs/1802.00420
      #code: https://github.com/anishathalye/obfuscated-gradients/tree/master/defensegan
-
  name: Towards the first adversarially robust neural network model on MNIST
  url: https://arxiv.org/abs/1805.09190
  authors: Schott et al.
  code: https://github.com/bethgelab/AnalysisBySynthesis
  venue: NeurIPS SECML 2018
  venue_date: 2018-12-07
  claims:
    - dataset: MNIST
      threat_model: $$\ell_2 (\epsilon = 1.5)$$
      natural: 99% accuracy
      claim: 80% accuracy
-
  name: Provable Robustness of ReLU networks via Maximization of Linear Regions
  url: https://arxiv.org/abs/1810.07481
  authors: Croce et al.
  code: https://github.com/max-andr/provable-robustness-max-linear-regions
  venue: AISTATS 2019
  venue_date: 2019-04-16
  claims:
    - dataset: MNIST
      threat_model: $$\ell_\infty (\epsilon = 0.1)$$
      natural: 98.81% accuracy
      claim: 96.4% accuracy (on first 1000 test points)
    - dataset: FMNIST
      threat_model: $$\ell_\infty (\epsilon = 0.1)$$
      natural: 85.50% accuracy
      claim: 73.4% accuracy (on first 1000 test points)
    - dataset: GTS
      threat_model: $$\ell_\infty (\epsilon = 0.2)$$
      natural: 84.65% accuracy
      claim: 67.9% accuracy (on first 1000 test points)
-
  name: Harnessing the Vulnerability of Latent Layers in Adversarially Trained Models
  url: https://arxiv.org/abs/1905.05186
  authors: Sinha et al.
  code: https://github.com/conference-submission-anon/LAT_adversarial_robustness
  venue: IJCAI 2019
  venue_date: 2019-08-10
  claims:
    - dataset: CIFAR&#8209;10
      threat_model: $$\ell_\infty (\epsilon = 0.03)$$
      natural: 87.8% accuracy
      claim: 53.82% accuracy
