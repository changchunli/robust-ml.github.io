-
  name: Deflecting Adversarial Attacks with Pixel Deflection
  url: https://arxiv.org/abs/1801.08926
  authors: Prakash et al.
  code: https://github.com/iamaaditya/pixel-deflection # TODO link to something that implements robustml
  venue: CVPR 2018
  venue_date: 2018-06-19
  dataset: ImageNet
  threat_model: $$\ell_2 (\epsilon = 0.05)$$
  claims: >
    81% accuracy (on images originally classified correctly)
  analyses:
    - name: "On the Robustness of the CVPR 2018 White-Box Adversarial Example Defenses"
      url: https://arxiv.org/abs/1804.03286
      code: https://github.com/carlini/pixel-deflection # TODO link to an attack that implements the robustml interface
-
  name: Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser
  url: https://arxiv.org/abs/1712.02976
  authors: Liao et al.
  code: https://github.com/lfz/Guided-Denoise # TODO link to something that implements robustml
  venue: CVPR 2018
  venue_date: 2018-06-19
  dataset: ImageNet
  threat_model: $$\ell_\infty (\epsilon = 4/255)$$
  claims: >
    75% accuracy
  analyses:
    - name: "On the Robustness of the CVPR 2018 White-Box Adversarial Example Defenses"
      url: https://arxiv.org/abs/1804.03286
      code: https://github.com/anishathalye/guided-denoise # TODO link to an attack that implements the robustml interface
-
  name: Towards Deep Learning Models Resistant to Adversarial Attacks
  url: https://arxiv.org/abs/1706.06083
  authors: Madry et al.
  code: https://github.com/MadryLab/cifar10_challenge
  venue: ICLR 2018
  venue_date: 2018-04-30
  dataset: CIFAR-10
  threat_model: $$\ell_\infty (\epsilon = 8/255)$$
  claims: >
    47% accuracy
  analyses:
-
  name: Mitigating Adversarial Effects Through Randomization
  url: https://arxiv.org/abs/1711.01991
  authors: Xie et al.
  code: https://github.com/anishathalye/obfuscated-gradients/tree/robustml/randomization
  venue: ICLR 2018
  venue_date: 2018-04-30
  dataset: ImageNet
  threat_model: $$\ell_\infty (\epsilon = 10/255)$$
  claims: >
    86% accuracy (on images originally classified correctly)
  analyses:
    - name: "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"
      url: https://arxiv.org/abs/1802.00420
      code: https://github.com/anishathalye/obfuscated-gradients/tree/robustml/randomization
-
  name: "Thermometer Encoding: One Hot Way To Resist Adversarial Examples"
  url: https://openreview.net/forum?id=S18Su--CW
  authors: Buckman et al.
  code: https://github.com/anishathalye/obfuscated-gradients/tree/master/thermometer # TODO link to a reimplementation that implements the robustml interface
  venue: ICLR 2018
  venue_date: 2018-04-30
  dataset: CIFAR-10
  threat_model: $$\ell_\infty (\epsilon = 8/255)$$
  claims: >
    79% accuracy
  analyses:
    - name: "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"
      url: https://arxiv.org/abs/1802.00420
      code: https://github.com/anishathalye/obfuscated-gradients/tree/master/thermometer # TODO link to attack that implements the robustml interface
-
  name: Countering Adversarial Images using Input Transformations
  url: https://arxiv.org/abs/1711.00117
  authors: Guo et al.
  code: https://github.com/facebookresearch/adversarial_image_defenses # TODO link to the one that implements the robustml interface
  venue: ICLR 2018
  venue_date: 2018-04-30
  dataset: ImageNet
  threat_model: $$\ell_2 (\epsilon = 0.06)$$
  claims: >
    70% accuracy on ImageNet with average normalized $$\ell_2$$ perturbation of
    0.06
  analyses:
    - name: "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"
      url: https://arxiv.org/abs/1802.00420
      code: https://github.com/anishathalye/obfuscated-gradients/tree/master/inputtransformations # TODO link to attack that implements the robustml interface
-
  name: Stochastic Activation Pruning for Robust Adversarial Defense
  url: https://arxiv.org/abs/1803.01442
  authors: Dhillon et al.
  code: https://github.com/anishathalye/obfuscated-gradients/tree/master/sap # TODO link to something that implements the robustml interface
  venue: ICLR 2018
  venue_date: 2018-04-30
  dataset: CIFAR-10
  threat_model: $$\ell_\infty (\epsilon = 8/255)$$
  claims: >
    80% accuracy
  analyses:
    - name: "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"
      url: https://arxiv.org/abs/1802.00420
      code: https://github.com/anishathalye/obfuscated-gradients/tree/master/sap # TODO link to attack that implements the robustml interface
-
  name: "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples"
  url: https://arxiv.org/abs/1710.10766
  authors: Song et al.
  code: https://github.com/anishathalye/obfuscated-gradients/tree/master/pixeldefend # TODO link to something that implements the robustml interface
  venue: ICLR 2018
  venue_date: 2018-04-30
  dataset: CIFAR-10
  threat_model: $$\ell_\infty (\epsilon = 8/255)$$
  claims: >
    70% accuracy
  analyses:
    - name: "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"
      url: https://arxiv.org/abs/1802.00420
      code: https://github.com/anishathalye/obfuscated-gradients/tree/master/pixeldefend # TODO link to attack that implements the robustml interface
-
  name: "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models"
  url: https://openreview.net/forum?id=BkJ3ibb0- # TODO is there an arxiv url?
  authors: Samangouei et al.
  code: https://github.com/anishathalye/obfuscated-gradients/tree/master/defensegan # TODO link to something that implements the robustml interface
  venue: ICLR 2018
  venue_date: 2018-04-30
  dataset: MNIST
  threat_model: $$\ell_\infty (\epsilon = 0.3)$$
  claims: >
    47% accuracy
  analyses:
    - name: "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"
      url: https://arxiv.org/abs/1802.00420
      code: https://github.com/anishathalye/obfuscated-gradients/tree/master/defensegan # TODO link to attack that implements the robustml interface
