-
  name: Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks
  url: https://arxiv.org/abs/1511.04508
  authors: Papernot et al.
  code: https://github.com/carlini/breaking_defensive_distillation # TODO link to something that implements robustml
  venue: S&P 2016
  venue_date: 2016-05-23
  dataset: CIFAR-10
  threat_model: $$\ell_0 (\epsilon = 112)$$
  natural: 81% accuracy
  claims: >
    17% adversary success rate in changing classifier's prediction
  analyses:
    - claims: 0% accuracy
      citation: CW16
      code: https://github.com/carlini/breaking_defensive_distillation # TODO link to something that implements robustml
-
  name: "MagNet: a Two-Pronged Defense against Adversarial Examples"
  url: https://arxiv.org/abs/1705.09064
  authors: Meng & Chen
  code: https://github.com/Trevillie/MagNet # TODO link to something that implements robustml
  venue: CCS 2017
  venue_date: 2017-10-30
  dataset: CIFAR-10
  threat_model: $$\ell_2 (\epsilon = 0.5)$$
  natural: 91% accuracy
  claims: >
    83% accuracy (on images originally classified correctly)
  analyses:
    - claims: 0% accuracy
      citation: CW17
      url: https://arxiv.org/abs/1711.08478
      code: https://github.com/carlini/MagNet # TODO link to something that implements robustml
-
  name: "Efficient Defenses Against Adversarial Attacks"
  url: https://arxiv.org/abs/1707.06728
  authors: Zantedeschi et al.
  code: https://github.com/carlini/breaking_efficient_defenses # TODO link to something that implements robustml
  venue: AISec 2017
  venue_date: 2017-11-03
  dataset: CIFAR-10
  threat_model: $$\ell_2 (\epsilon = \text{unspecified})$$
  natural: 75% accuracy
  claims: >
    60% accuracy
  analyses:
    - claims: 0% accuracy
      citation: CW17
      url: https://arxiv.org/abs/1711.08478
      code: https://github.com/carlini/breaking_efficient_defenses # TODO link to something that implements robustml
-
  name: Deflecting Adversarial Attacks with Pixel Deflection
  url: https://arxiv.org/abs/1801.08926
  authors: Prakash et al.
  code: https://github.com/iamaaditya/pixel-deflection # TODO link to something that implements robustml
  venue: CVPR 2018
  venue_date: 2018-06-19
  dataset: ImageNet
  threat_model: $$\ell_2 (\epsilon = 0.05)$$
  natural: 98.9% accuracy (on images originally classified correctly by underlying model)
  claims: >
    81% accuracy (on images originally classified correctly)
  analyses:
    - claims: 0% accuracy
      citation: AC18
      url: https://arxiv.org/abs/1804.03286
      code: https://github.com/carlini/pixel-deflection # TODO link to an attack that implements the robustml interface
-
  name: Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser
  url: https://arxiv.org/abs/1712.02976
  authors: Liao et al.
  code: https://github.com/lfz/Guided-Denoise # TODO link to something that implements robustml
  venue: CVPR 2018
  venue_date: 2018-06-19
  dataset: ImageNet
  threat_model: $$\ell_\infty (\epsilon = 4/255)$$
  natural: 75% accuracy
  claims: >
    75% accuracy
  analyses:
    - claims: 0% accuracy
      citation: AC18
      url: https://arxiv.org/abs/1804.03286
      code: https://github.com/anishathalye/guided-denoise # TODO link to an attack that implements the robustml interface
-
  name: Towards Deep Learning Models Resistant to Adversarial Attacks
  url: https://arxiv.org/abs/1706.06083
  authors: Madry et al.
  code: https://github.com/MadryLab/cifar10_challenge
  venue: ICLR 2018
  venue_date: 2018-04-30
  dataset: CIFAR-10
  threat_model: $$\ell_\infty (\epsilon = 8/255)$$
  natural: 87% accuracy
  claims: >
    46% accuracy
  analyses:
-
  name: Mitigating Adversarial Effects Through Randomization
  url: https://arxiv.org/abs/1711.01991
  authors: Xie et al.
  code: https://github.com/anishathalye/obfuscated-gradients/tree/robustml/randomization
  venue: ICLR 2018
  venue_date: 2018-04-30
  dataset: ImageNet
  threat_model: $$\ell_\infty (\epsilon = 10/255)$$
  natural: 99.2% accuracy (on images originally classified correctly by underlying model)
  claims: >
    86% accuracy (on images originally classified correctly)
  analyses:
    - claims: 0% accuracy
      citation: ACW18
      url: https://arxiv.org/abs/1802.00420
      code: https://github.com/anishathalye/obfuscated-gradients/tree/robustml/randomization
-
  name: "Thermometer Encoding: One Hot Way To Resist Adversarial Examples"
  url: https://openreview.net/forum?id=S18Su--CW
  authors: Buckman et al.
  code: https://github.com/anishathalye/obfuscated-gradients/tree/master/thermometer # TODO link to a reimplementation that implements the robustml interface
  venue: ICLR 2018
  venue_date: 2018-04-30
  dataset: CIFAR-10
  threat_model: $$\ell_\infty (\epsilon = 8/255)$$
  natural: 90% accuracy
  claims: >
    79% accuracy
# adversarially trained variant
  analyses:
    - claims: 30% accuracy
      citation: ACW18
      url: https://arxiv.org/abs/1802.00420
      code: https://github.com/anishathalye/obfuscated-gradients/tree/master/thermometer # TODO link to attack that implements the robustml interface
-
  name: Countering Adversarial Images using Input Transformations
  url: https://arxiv.org/abs/1711.00117
  authors: Guo et al.
  code: https://github.com/facebookresearch/adversarial_image_defenses # TODO link to the one that implements the robustml interface
  venue: ICLR 2018
  venue_date: 2018-04-30
  dataset: ImageNet
  threat_model: $$\ell_2 (\epsilon = 0.06)$$
  natural: 75% accuracy
  claims: >
    70% accuracy on ImageNet with average normalized $$\ell_2$$ perturbation of
    0.06
  analyses:
    - claims: 0% accuracy
      citation: ACW18
      url: https://arxiv.org/abs/1802.00420
      code: https://github.com/anishathalye/obfuscated-gradients/tree/master/inputtransformations # TODO link to attack that implements the robustml interface
-
  name: Stochastic Activation Pruning for Robust Adversarial Defense
  url: https://arxiv.org/abs/1803.01442
  authors: Dhillon et al.
  code: https://github.com/anishathalye/obfuscated-gradients/tree/master/sap # TODO link to something that implements the robustml interface
  venue: ICLR 2018
  venue_date: 2018-04-30
  dataset: CIFAR-10
  threat_model: $$\ell_\infty (\epsilon = 4/255)$$
  natural: 83% accuracy
  claims: >
    51% accuracy
  analyses:
    - claims: 0% accuracy
      citation: ACW18
      url: https://arxiv.org/abs/1802.00420
      code: https://github.com/anishathalye/obfuscated-gradients/tree/master/sap # TODO link to attack that implements the robustml interface
-
  name: "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples"
  url: https://arxiv.org/abs/1710.10766
  authors: Song et al.
  code: https://github.com/anishathalye/obfuscated-gradients/tree/master/pixeldefend # TODO link to something that implements the robustml interface
  venue: ICLR 2018
  venue_date: 2018-04-30
  dataset: CIFAR-10
  threat_model: $$\ell_\infty (\epsilon = 8/255)$$
  natural: 90% accuracy
  claims: >
    70% accuracy
  analyses:
    - claims: 9% accuracy
      citation: ACW18
      url: https://arxiv.org/abs/1802.00420
      code: https://github.com/anishathalye/obfuscated-gradients/tree/master/pixeldefend # TODO link to attack that implements the robustml interface
-
  name: "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models"
  url: https://openreview.net/forum?id=BkJ3ibb0- # TODO is there an arxiv url?
  authors: Samangouei et al.
  code: https://github.com/anishathalye/obfuscated-gradients/tree/master/defensegan # TODO link to something that implements the robustml interface
  venue: ICLR 2018
  venue_date: 2018-04-30
  dataset: MNIST
  threat_model: $$\ell_2 (\epsilon = \text{unspecified})$$
  natural: 99.7% accuracy
  claims: >
    92% accuracy
  analyses:
    - claims: 55% accuracy
      citation: ACW18
      url: https://arxiv.org/abs/1802.00420
      code: https://github.com/anishathalye/obfuscated-gradients/tree/master/defensegan # TODO link to attack that implements the robustml interface
