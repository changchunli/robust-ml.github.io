# you can use Markdown syntax in here
-
  defense: >
    [Towards Deep Learning Models Resistant to Adversarial
    Attacks](https://arxiv.org/abs/1706.06083) (Madry et al.)
  venue: ICLR 2018
  venue_date: 2018-04-30
  category: Adversarial training
  code: >
    [available](https://github.com/MadryLab/cifar10_challenge)
  summary: >
    Adversarial training on PGD adversarial examples
  claims: >
    47% accuracy on CIFAR-10 with $$\ell_\infty$$ perturbation of $$\epsilon = 8/255$$
  break: unknown
-
  defense: >
    [Thermometer Encoding: One Hot Way To Resist Adversarial
    Examples](https://openreview.net/forum?id=S18Su--CW) (Buckman et al.)
  venue: ICLR 2018
  venue_date: 2018-04-30
  category: Non-differentiability
  code: unavailable
  summary: >
    Input discretization using unary encoding
  claims: >
    79% accuracy on CIFAR-10 with $$\ell_\infty$$ perturbation of $$\epsilon = 8/255$$
  break: >
    [Obfuscated Gradients Give a False Sense of Security: Circumventing
    Defenses to Adversarial Examples](https://arxiv.org/abs/1802.00420)
    ([code](https://github.com/anishathalye/obfuscated-gradients))
-
  defense: >
    [Characterizing Adversarial Subspaces Using Local Intrinsic
    Dimensionality](https://arxiv.org/abs/1801.02613) (Ma et al.)
  venue: ICLR 2018
  venue_date: 2018-04-30
  category: Misc
  code: >
    [available](https://github.com/xingjunm/lid_adversarial_subspace_detection)
  summary: >
    Using Local Intrinsic Dimensionality (LID) to detect adversarial examples
  claims: >
    95.7% attack failure rate on CIFAR-10 with unspecified perturbation bound
  break: >
    [Obfuscated Gradients Give a False Sense of Security: Circumventing
    Defenses to Adversarial Examples](https://arxiv.org/abs/1802.00420)
    ([code](https://github.com/anishathalye/obfuscated-gradients))
-
  defense: >
    [Countering Adversarial Images using Input
    Transformations](https://arxiv.org/abs/1711.00117) (Guo et al.)
  venue: ICLR 2018
  venue_date: 2018-04-30
  category: Input transformations
  code: >
    [available](https://github.com/facebookresearch/adversarial_image_defenses)
  summary: >
    Transforming the input (randomly or non-differentiably) before classifying it
  claims: >
    70% accuracy on ImageNet with average normalized $$\ell_2$$ perturbation of
    0.06
  break: >
    [Obfuscated Gradients Give a False Sense of Security: Circumventing
    Defenses to Adversarial Examples](https://arxiv.org/abs/1802.00420)
    ([code](https://github.com/anishathalye/obfuscated-gradients))
